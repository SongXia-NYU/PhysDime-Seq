********************
cutoffP_BN: torch.Size([])
centersP_BN: torch.Size([64])
widthsP_BN: torch.Size([64])
cutoffP-noOut_BN: torch.Size([])
centersP-noOut_BN: torch.Size([64])
widthsP-noOut_BN: torch.Size([64])
scale: torch.Size([95, 2])
shift: torch.Size([95, 2])
embedding_layer.embedding.weight: torch.Size([95, 160])
module0.interaction.u: torch.Size([1, 160])
module0.interaction.message_pass_layer.lin_for_same.weight: torch.Size([160, 160])
module0.interaction.message_pass_layer.lin_for_same.bias: torch.Size([160])
module0.interaction.message_pass_layer.lin_for_diff.weight: torch.Size([160, 160])
module0.interaction.message_pass_layer.lin_for_diff.bias: torch.Size([160])
module0.interaction.message_pass_layer.G.weight: torch.Size([160, 64])
module0.interaction.res_layer0.lin1.weight: torch.Size([160, 160])
module0.interaction.res_layer0.lin1.bias: torch.Size([160])
module0.interaction.res_layer0.lin2.weight: torch.Size([160, 160])
module0.interaction.res_layer0.lin2.bias: torch.Size([160])
module0.interaction.lin_last.weight: torch.Size([160, 160])
module0.interaction.lin_last.bias: torch.Size([160])
module0.res_layer0.lin1.weight: torch.Size([160, 160])
module0.res_layer0.lin1.bias: torch.Size([160])
module0.res_layer0.lin2.weight: torch.Size([160, 160])
module0.res_layer0.lin2.bias: torch.Size([160])
module0.output.res_layer0.lin1.weight: torch.Size([160, 160])
module0.output.res_layer0.lin1.bias: torch.Size([160])
module0.output.res_layer0.lin2.weight: torch.Size([160, 160])
module0.output.res_layer0.lin2.bias: torch.Size([160])
module0.output.lin.weight: torch.Size([2, 160])
module1.interaction.u: torch.Size([1, 160])
module1.interaction.message_pass_layer.lin_for_same.weight: torch.Size([160, 160])
module1.interaction.message_pass_layer.lin_for_same.bias: torch.Size([160])
module1.interaction.message_pass_layer.lin_for_diff.weight: torch.Size([160, 160])
module1.interaction.message_pass_layer.lin_for_diff.bias: torch.Size([160])
module1.interaction.message_pass_layer.G.weight: torch.Size([160, 64])
module1.interaction.res_layer0.lin1.weight: torch.Size([160, 160])
module1.interaction.res_layer0.lin1.bias: torch.Size([160])
module1.interaction.res_layer0.lin2.weight: torch.Size([160, 160])
module1.interaction.res_layer0.lin2.bias: torch.Size([160])
module1.interaction.lin_last.weight: torch.Size([160, 160])
module1.interaction.lin_last.bias: torch.Size([160])
module1.res_layer0.lin1.weight: torch.Size([160, 160])
module1.res_layer0.lin1.bias: torch.Size([160])
module1.res_layer0.lin2.weight: torch.Size([160, 160])
module1.res_layer0.lin2.bias: torch.Size([160])
module1.output.res_layer0.lin1.weight: torch.Size([160, 160])
module1.output.res_layer0.lin1.bias: torch.Size([160])
module1.output.res_layer0.lin2.weight: torch.Size([160, 160])
module1.output.res_layer0.lin2.bias: torch.Size([160])
module1.output.lin.weight: torch.Size([2, 160])
module2.interaction.u: torch.Size([1, 160])
module2.interaction.message_pass_layer.lin_for_same.weight: torch.Size([160, 160])
module2.interaction.message_pass_layer.lin_for_same.bias: torch.Size([160])
module2.interaction.message_pass_layer.lin_for_diff.weight: torch.Size([160, 160])
module2.interaction.message_pass_layer.lin_for_diff.bias: torch.Size([160])
module2.interaction.message_pass_layer.G.weight: torch.Size([160, 64])
module2.interaction.res_layer0.lin1.weight: torch.Size([160, 160])
module2.interaction.res_layer0.lin1.bias: torch.Size([160])
module2.interaction.res_layer0.lin2.weight: torch.Size([160, 160])
module2.interaction.res_layer0.lin2.bias: torch.Size([160])
module2.interaction.lin_last.weight: torch.Size([160, 160])
module2.interaction.lin_last.bias: torch.Size([160])
module2.res_layer0.lin1.weight: torch.Size([160, 160])
module2.res_layer0.lin1.bias: torch.Size([160])
module2.res_layer0.lin2.weight: torch.Size([160, 160])
module2.res_layer0.lin2.bias: torch.Size([160])
module2.output.res_layer0.lin1.weight: torch.Size([160, 160])
module2.output.res_layer0.lin1.bias: torch.Size([160])
module2.output.res_layer0.lin2.weight: torch.Size([160, 160])
module2.output.res_layer0.lin2.bias: torch.Size([160])
module2.output.lin.weight: torch.Size([2, 160])
********************
train data index:tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
        54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,
        72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,
        90, 91, 92, 93, 94, 95, 96, 97, 98, 99]) ...
val data index:tensor([99000, 99001, 99002, 99003, 99004, 99005, 99006, 99007, 99008, 99009,
        99010, 99011, 99012, 99013, 99014, 99015, 99016, 99017, 99018, 99019,
        99020, 99021, 99022, 99023, 99024, 99025, 99026, 99027, 99028, 99029,
        99030, 99031, 99032, 99033, 99034, 99035, 99036, 99037, 99038, 99039,
        99040, 99041, 99042, 99043, 99044, 99045, 99046, 99047, 99048, 99049,
        99050, 99051, 99052, 99053, 99054, 99055, 99056, 99057, 99058, 99059,
        99060, 99061, 99062, 99063, 99064, 99065, 99066, 99067, 99068, 99069,
        99070, 99071, 99072, 99073, 99074, 99075, 99076, 99077, 99078, 99079,
        99080, 99081, 99082, 99083, 99084, 99085, 99086, 99087, 99088, 99089,
        99090, 99091, 99092, 99093, 99094, 99095, 99096, 99097, 99098, 99099]) ...
n_atom_embedding = 95
n_feature = 160
n_output = 2
n_dime_before_residual = 1
n_dime_after_residual = 2
n_output_dense = 3
n_phys_atomic_res = 1
n_phys_interaction_res = 1
n_phys_output_res = 1
n_bi_linear = 8
nh_lambda = 0.01
normalize = True
debug_mode = True
activations = ssp ssp ssp
shared_normalize_param = True
restrain_non_bond_pred = True
expansion_fn = (P_BN,P-noOut_BN):gaussian_64_10.0 C_BN:coulomb_10.0
modules = P-noOut P-noOut P C
bonding_type = BN BN BN BN
uncertainty_modify = none
energy_shift = -4.112745206910419
energy_scale = 0.32129832484009635
